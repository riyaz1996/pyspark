{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4333e6-c62b-4071-b7d5-ef50282dddb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.key.riyazsparkpractice24.dfs.core.windows.net\",\n",
    "               \"f/qu7WlrAu/vakDff9iixYzUyAVj+MmxkDdA5xHDDqa2yi2hY52z63t8lkBfy/j7MuZZ4+Ox5/Cj+AStmiJsyw==\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9124931-e8db-4075-9788-55e9da47c9c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(dbutils.fs.ls(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/ml-100k/u.data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3e8f69-9a2c-4ba8-9249-1c4cb2ec6dbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/ml-100k/u.data\")\n",
    "\n",
    "rdd = df.rdd\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "\n",
    "result = ratings.countByValue()\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "print(type(sortedResults))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afb5717-9949-4a39-8322-45172ae1c936",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/fakefriends.csv\")\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "\n",
    "def parseline(x):\n",
    "    age = int(x.split(',')[2])\n",
    "    number_of_friends = int(x.split(',')[3])\n",
    "    return(age,number_of_friends)\n",
    "\n",
    "lines_splitted = lines.map(parseline)\n",
    "\n",
    "average_friends_by_age = lines_splitted.mapValues(lambda x:(x,1)).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\\\n",
    "    .mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "average_friends_by_age  = average_friends_by_age.sortByKey()\n",
    "\n",
    "for i in average_friends_by_age.collect():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03710b3f-a160-49f7-8d35-7eca113b7a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/1800.csv\")\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "\n",
    "def parseline(x):\n",
    "    stationid = x.split(',')[0]\n",
    "    entrytype = x.split(',')[2]\n",
    "    temprature = float(x.split(',')[3])*0.1*(9.0/5.0)+32.0\n",
    "    return(stationid,entrytype,temprature)\n",
    "\n",
    "parsedlines = lines.map(parseline)\n",
    "\n",
    "filtered_rdd = parsedlines.filter(lambda x: x[1]=='TMIN')\n",
    "\n",
    "stationtemps = filtered_rdd.map(lambda x :(x[0],x[2]))\n",
    "\n",
    "mintemps = stationtemps.reduceByKey(lambda x,y:min(x,y))\n",
    "\n",
    "for i in mintemps.collect():\n",
    "    print(i[0],\"\\t{:.2f}F\".format(i[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921a2791-8621-422e-bf9a-441dab138311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/1800.csv\")\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "\n",
    "def parseline(x):\n",
    "    stationid = x.split(',')[0]\n",
    "    entrytype = x.split(',')[2]\n",
    "    temprature = float(x.split(',')[3])*0.1*(9.0/5.0)+32.0\n",
    "    return(stationid,entrytype,temprature)\n",
    "\n",
    "parsedlines = lines.map(parseline)\n",
    "\n",
    "filtered_rdd = parsedlines.filter(lambda x: x[1]=='TMAX')\n",
    "\n",
    "stationtemps = filtered_rdd.map(lambda x :(x[0],x[2]))\n",
    "\n",
    "mintemps = stationtemps.reduceByKey(lambda x,y:max(x,y))\n",
    "\n",
    "for i in mintemps.collect():\n",
    "    print(i[0],\"\\t{:.2f}F\".format(i[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ce4df2-5ced-463b-b7cf-8e5106a2c832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/Book\")\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "\n",
    "words = lines.flatMap(lambda x:x.split())\n",
    "wordcount = words.countByValue()\n",
    "\n",
    "for i,j in wordcount.items(a):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee0c2ae-49da-40ee-85d7-852ac071b91e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/Book\")\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "\n",
    "\n",
    "def normalizewords(x):\n",
    "    return re.compile(r'\\W+',re.UNICODE).split(x.lower())\n",
    "\n",
    "words = lines.flatMap(normalizewords)\n",
    "wordcount = words.countByValue()\n",
    "\n",
    "for i in wordcount.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da73acc4-f8fe-4498-8155-f7cab8157829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/Book\")\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "\n",
    "def normalizewords(x):\n",
    "    return re.compile(r'\\W+',re.UNICODE).split(x.lower())\n",
    "\n",
    "words = lines.flatMap(normalizewords)\n",
    "\n",
    "wordcount = words.map(lambda x : (x,1)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0]))\n",
    "\n",
    "for i in wordcount.collect():\n",
    "    print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43325e37-f1a7-4844-a066-ae89f473a458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "df = spark.read.text(\"abfss://tamingbigdatawithpyspark@riyazsparkpractice24.dfs.core.windows.net/customer-orders.csv\")\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "lines = rdd.map(lambda row: row['value'])\n",
    "\n",
    "def parseline(a):\n",
    "    customer_id = int(a.split(',')[0])\n",
    "    amount_spent = float(a.split(',')[2])\n",
    "    return(customer_id,amount_spent)\n",
    "\n",
    "customer_spent = lines.map(parseline)\n",
    "total_spend_by_customer = customer_spent.reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0]))\n",
    "\n",
    "for i in total_spend_by_customer.collect():\n",
    "    print(i[0],\"\\t{:.2f}\".format(i[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7924c35e-4ccc-4b40-a11b-fa2a6ad3eaed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Taming_bigData_with_pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
